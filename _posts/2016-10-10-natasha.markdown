---
layout: post
title: Generating Music by Tuning Recurrent Neural Networks with Reinforcement Learning
date:   2016-10-24 08:00:00 -0700
author: natashajaques
tags: magenta,melody,rnn,lstm,reinforcement,music
excerpt_separator: <!--more-->
---

When I joined [Magenta][magenta] as an intern this summer, the team was hard at work on developing better ways to train Recurrent Neural Networks (RNNs) to generate sequences of notes. As you may remember from [previous posts][elliot post], these models typically consist of a Long Short-Term Memory (LSTM) network trained on monophonic melodies. This means that melodies are fed into the network one note at a time, and it is trained to predict the next note in the sequence. The figure below shows a simplified version of this type of network unrolled over time, in which it is being trained on the first 6 notes of "Twinkle Twinkle Little Star". From now on, I'll refer to this type of model as a **Note RNN**.

![Note RNN model](/assets/rl_rnn/note_rnn_model.png "Note RNN model")



Frustrating limitation of LSTMs
Check out this character RNN producing sentences
Sentences lose track of where they were going, paragraphs are not about a consistent topic
This happens with music too... melodies wander and do not repeat, lack a consistent global structure

I thought to myself that this was an unfair expectation. 
Any human music student gets taught concepts of music theory!
Another way to look at this is that music is an interesting test-bed for sequence generation, because musical compositions adhere to a relatively well-defined set of structural rules. 

Reinforcement learning can be used to 
non differentiable
points in a game 

This post will give an introduction to the **SWEETNAMEHERE** model. [Code](our code) to run this model has recently been posted to the Magenta github; the code will be explained in detail at the end of this post in case you would like to try it for yourself! Finally, there is also a recent [arxiv paper](our arxiv) describing this work in detail, written by myself, Shane Gu, Richard E. Turner, and Douglas Eck. A version of this work was accepted at the [NIPS 2016 Deep Reinforcement Learning Workshop](nips rl). 

Introduction to Deep Q-Learning
---------------------
In reinforcement learning (RL), an agent interacts with an environment. Given the state of the enviornment $$s$$, the agent takes an action $$a$$, receives a reward $$r$$, and the environment transitions to a new state, $$s'$$. The goal of the agent is to maximize reward, which is usually some clear signal from the environment, such as points in a game. 

The rules for how the agent chooses to act in the environment define a *policy*. To learn the most effective policy, the agent cannot simply greedily maximize the reward it will receive after the next action, but must instead consider the total cumulative reward it can expect to receive over a course of actions occurring in the future. Because future rewards are typically uncertain if the environment has random effects, a discount factor of $$\gamma$$ is applied to the reward for each timestep in the future. If $$r_t$$ is the reward received at timestep $$t$$, then $$R_t$$ is the total future discounted *return*:
\begin{align}
R_t = \sum^T_{t'=t}\gamma^{t'-t}r_{t'} 
\end{align}

In Q-learning, the goal is to learn a Q function that gives the maximum expected discounted future return for taking any action $$a$$ in state $$s$$, and continuing to act optimally at each step in the future. Therefore the optimal Q function, $$Q^*$$ is defined as:

$$\begin{align}
Q^*(s, a) = max_\pi \mathbb{E}[R_t|s_t = s, a_t = a, \pi]
\end{align}$$

where $$\pi$$ is a policy mapping states to distributions over actions. To learn $$Q*$$, we can apply an iterative update based on the Bellman equation:
\begin{align}
Q_{i+1}(s, a) = \mathbb{E}[r + \gamma max_{a'}Q_i(s',a')|s,a]
\end{align}
where *r* is the reward received for taking action *a* in state *s*. This value iteration method will converge to $$Q^*$$ as $$i \rightarrow \infty$$ \cite{sutton}.

In Deep Q Learning (DQL) \cite{dqn}, a neural network called the *Q-network* is used to approximate the Q function, $$Q(s, a; \theta) \approx Q^*(s, a)$$. The network parameters $$\theta$$ are learned by applying stochastic gradient descent (SGD) updates with respect to the following loss function:
\begin{align} 
\label{eq:qloss}
L_t(\theta_t) = (Q(s,a;\theta_t) - (r_t + \gamma \max_{a'}Q(s',a';\theta_{t-1}))^2
\end{align}
The loss function describes the difference between the expected future discounted return output by the *Q-network* at step $$t$$, and the actual reward received at step $$t$$ ($$r_t$$) plus the discounted future return estimated by the *Q-network* parameters at step $$t-1$$; essentially, the prediction error in estimating the Q function. Importantly, the parameters for the previous generation of the network ($$\theta_{t-1}$$) are held fixed, and not updated by SGD.

While interacting with the environment, the $$Q$$-learning agent typically follows an $$\epsilon$$-greedy policy. This means that it will greedily choose the action with the highest Q value with probability $$1-\epsilon$$, and choose a random action with probability $$\epsilon$$. This off-policy learning method ensures a reasonably trade-off between exploration of the action space, and exploitation of the most efficient strategies learned so far. 

Several techniques are required for DQL to work effectively. As the agent interacts with the environment, $$<s_t,a_t,r_t,s_{t+1}>$$ tuples it experiences are stored in a *replay memory*. Training the *Q-network* is accomplished by randomly sampling batches from the *replay memory* to compute the loss. The *replay memory* is essential for learning; if the agent was trained using consecutive samples of experience (as in online Q-learning), the samples would be highly correlated, updates would have high variance, and the network parameters could become stuck in a local minimum or diverge \cite{dqn}. 

Further optimizations to the DQL algorithm have been proposed that help enhance learning and ensure stability. One of these is Deep Double Q-learning \cite{dqqn}, in which a second, *Target Q-network* is used to estimate expected future return, while the *Q-network* is used to $$\epsilon$$-greedily choose the next action. Since Q-learning has been shown to learn unrealistically high action values because it estimates maximum expected return, having a second Q-network can lead to more realistic estimates and better performance \cite{dqqn}.

Tune DQN
----------

![Model diagram](/assets/rl_rnn/rl_rnn_diagram.png "Model diagram")

The total reward given at time $$t$$ is therefore: 

$$\begin{align} 
\label{eq:reward}
r(s,a) = \log p(a|s) + \frac{1}{c}r_{MT}(a,s)
\end{align}$$

where $$c$$ is a constant controlling the emphasis placed on the music theory reward. Given the DQN loss function in Eq. ? and modified reward function in Eq. ?, the new loss function and learned policy for our model are,

$$\begin{align}
\label{eq:melody_dqn_loss}
&L(\theta)= \mathbb{E}_\beta[(\log p(a|s) + \frac{1}{c}r_{MT}(a,s) + \gamma \max_{a'}Q(s',a';\theta^-) - Q(s,a;\theta))^2]\\
\end{align}$$

Thus, the modified loss function
forces the model to learn that the most valuable actions in terms of expected future rewards are those that conform to the music theory rules, but still have the high probability in the original data.


Music Theory Rewards
--------------------

* **Stay in key**: Notes should belong to the same key. For example, if the desired key is C-major, a B-flat would not be an acceptable note. 

* **Begin and end with the tonic note**: The first note of the composition, and the first note of the final bar should be the tonic note of the key; e.g. if the key is C-major, this note would be middle C, or 14 in our encoding. 

* **Avoid excessively repeated notes**: Unless a rest is introduced or a note is held, a single tone should not be repeated more than four times in a row. While the number four can be considered a rough heuristic, avoiding excessively repeated notes and static melodic contours is Gauldin's first rule of melodic composition \cite{gauldin1995practical}.

* **Prefer harmonious intervals**: The composition should avoid awkward intervals like augmented sevenths, or large jumps of more than an octave. Since "a mixture of motion by both step and leap is preferred, with some tendency towards the former", both small steps and larger harmonic intervals are rewarded, but the reward for smaller steps is somewhat higher. 

*  **Resolve large leaps**: When the composition leaps several pitches in one direction, it should eventually be resolved by a leap back or gradual movement in the opposite direction. Leaping twice in the same direction is negatively rewarded. 

*  **Avoid continuously repeating extrema notes**: The highest note of the composition should be unique, as should the lowest note. 

*  **Avoid high auto-correlation**: To encourage variety, the reward function penalizes a melody if it is highly correlated with itself at a lag of 1, 2, or 3 beats. 

*  **Play motifs**: A musical motif is a succession of notes representing the shortest musical "idea"; in our implementation, it is defined as a bar of music with three or more unique notes. 

*  **Play repeated motifs**: Because repetition has been shown to be key to emotional engagement with music \cite{livingstone}, we sought to train the model to repeat motifs that it had previously introduced. 


Results
----------

| Undesirable behavior              | Note RNN | Q         | $$\Psi$$  | G         |
|-----------------------------------|----------|-----------|-----------|-----------|
| Notes excessively repeated        | 63.3%    | **0.0%**  | **0.02%** | **0.03%** |
| Notes not in key                  | 0.1%     | 1.0%      | 0.6%      | 28.7%     |
| Mean autocorrelation - lag 1      | -.16     | **-.11**  | **-.10**  | .55       |
| Mean autocorrelation - lag 2      | .14      | **.03**   | **-.01**  | .31       |
| Mean autocorrelation - lag 3      | -.13     | **.03**   | **.01**   | 17        |

| Desirable behavior                | Note RNN | Q         | $$\Psi$$  | G         |
|-----------------------------------|----------|-----------|-----------|-----------|
| Leaps resolved                    | 77.2%    | **91.1%** | **90.0%** | 52.2%     |
| Compositions starting with tonic  | 0.9%     | **28.8%** | **28.7%** | 0.0%      |
| Compositions with unique max note | 64.7%    | 56.4%     | 59.4%     | 37.1%     |
| Compositions with unique min note | 49.4%    | 51.9%     | **58.3%** | **56.5%** |
| Notes in motif                    | 5.9%     | **75.7%** | **73.8%** | **69.3%** |
| Notes in repeated motif           | 0.007%   | **0.11%** | **0.09%** | **0.01%** |

| ![](/assets/rl_rnn/rewards_note_rnn.png)  | ![](/assets/rl_rnn/rewards_music_theory.png) | 
|:---:|:---:|
| Note RNN rewards | Music theory rewards |

| ![](/assets/rl_rnn/note_rnn.png)  | ![](/assets/rl_rnn/q.png) | ![](/assets/rl_rnn/psi.png) | ![](/assets/rl_rnn/g.png) |
|:---:|:---:|:---:|:---:|
| Note RNN | Q | $$\Psi$$ | G |

How to use the code
-------------------


Acknowledgements
----------------
This work was done in collaboration with Shane Gu, Richard E. Turner, and [Douglas Eck][doug]. Many thanks also go to my wonderful collaborators on the [Magenta][magenta] team in [Google Brain][brain], and in particular [Kyle Kastner][kastner] for his knowledgeable insights and handy spectrogram-movie-producing code. 

[our arxiv]: https://arxiv.org/abs/comingsoon

[elliot post]: https://magenta.tensorflow.org/2016/07/15/lookback-rnn-attention-rnn/
[model code]: https://github.com/tensorflow/magenta/tree/master/magenta/models/SWEETNAMEHERE
[nips rl]: https://sites.google.com/site/deeprlnips2016/
[doug]: http://research.google.com/pubs/author39086.html
[kastner]: http://kastnerkyle.github.io/
[brain]: http://research.google.com/teams/brain/
[tensorflow]: https://www.tensorflow.org/
[magenta]: https://magenta.tensorflow.org/

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

